{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 301,966 tokens\n",
      "val has 36,059 tokens\n"
     ]
    }
   ],
   "source": [
    "%run model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, config, model, train_dataset):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.optimizer = None\n",
    "        self.train_dataset = train_dataset\n",
    "        self.callbacks = defaultdict(list)\n",
    "        self.device = config.device\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "        # variables that will be assigned to trainer class later for logging and etc\n",
    "        self.iter_num = 0\n",
    "        self.iter_time = 0.0\n",
    "        self.iter_dt = 0.0\n",
    "\n",
    "    def add_callback(self, onevent: str, callback):\n",
    "        self.callbacks[onevent].append(callback)\n",
    "\n",
    "    def set_callback(self, onevent: str, callback):\n",
    "        self.callbacks[onevent] = [callback]\n",
    "\n",
    "    def trigger_callbacks(self, onevent: str):\n",
    "        for callback in self.callbacks.get(onevent, []):\n",
    "            callback(self)\n",
    "\n",
    "    def run(self):\n",
    "        model, config = self.model, self.config\n",
    "\n",
    "        # setup the optimizer\n",
    "        self.optimizer = model.configure_optimizers(config)\n",
    "\n",
    "        # setup the dataloader\n",
    "        train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            sampler=torch.utils.data.RandomSampler(self.train_dataset, replacement=True, num_samples=int(1e10)),\n",
    "            shuffle=False,\n",
    "            # pin_memory=True,\n",
    "            batch_size=config.batch_size,\n",
    "            num_workers=config.num_workers,\n",
    "        )\n",
    "\n",
    "        model.train()\n",
    "        self.iter_num = 0\n",
    "        self.iter_time = time.time()\n",
    "        data_iter = iter(train_loader)\n",
    "        while True:\n",
    "\n",
    "            # fetch the next batch (x, y) and re-init iterator if needed\n",
    "            try:\n",
    "                batch = next(data_iter)\n",
    "            except StopIteration:\n",
    "                data_iter = iter(train_loader)\n",
    "                batch = next(data_iter)\n",
    "            batch = [t.to(self.device) for t in batch]\n",
    "            x, y = batch\n",
    "\n",
    "            # forward the model\n",
    "            logits, self.loss = model(x, y)\n",
    "\n",
    "            # backprop and update the parameters\n",
    "            model.zero_grad(set_to_none=True)\n",
    "            self.loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.trigger_callbacks('on_batch_end')\n",
    "            self.iter_num += 1\n",
    "            tnow = time.time()\n",
    "            self.iter_dt = tnow - self.iter_time\n",
    "            self.iter_time = tnow\n",
    "\n",
    "            # termination conditions\n",
    "            if config.max_iters is not None and self.iter_num >= config.max_iters:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTConfig:\n",
    "    def __init__(self, vocab_size, **kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "class CustomConfig(GPTConfig):\n",
    "    n_layer = 8\n",
    "    n_head = 8\n",
    "    n_embd = 256\n",
    "    embd_pdrop = 0.1\n",
    "    resid_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "    dropout = 0.1\n",
    "    compile = True\n",
    "    device = 'cuda'\n",
    "    num_workers = 0\n",
    "    max_iters = 2e4\n",
    "    batch_size = 4\n",
    "    block_size = 64\n",
    "    learning_rate = 6e-4\n",
    "    betas = (0.9, 0.95)\n",
    "    weight_decay = 1e-1\n",
    "    grad_norm_clip = 1.0\n",
    "\n",
    "vocab_size = len(train_ids)\n",
    "config = CustomConfig(vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 83.64M\n",
      "iter_dt 0.00ms; iter 0: train loss 5.14950\n",
      "iter_dt 34.36ms; iter 500: train loss 4.63397\n",
      "iter_dt 33.56ms; iter 1000: train loss 4.19386\n",
      "iter_dt 34.42ms; iter 1500: train loss 5.01648\n",
      "iter_dt 34.47ms; iter 2000: train loss 4.15115\n",
      "iter_dt 34.44ms; iter 2500: train loss 4.45165\n",
      "iter_dt 34.45ms; iter 3000: train loss 4.13863\n",
      "iter_dt 34.43ms; iter 3500: train loss 4.23959\n",
      "iter_dt 34.51ms; iter 4000: train loss 4.50570\n",
      "iter_dt 34.47ms; iter 4500: train loss 3.18282\n",
      "iter_dt 34.48ms; iter 5000: train loss 4.10450\n",
      "iter_dt 34.45ms; iter 5500: train loss 3.56379\n",
      "iter_dt 34.43ms; iter 6000: train loss 4.21151\n",
      "iter_dt 34.48ms; iter 6500: train loss 4.54891\n",
      "iter_dt 34.39ms; iter 7000: train loss 4.05246\n",
      "iter_dt 34.43ms; iter 7500: train loss 4.15091\n",
      "iter_dt 34.39ms; iter 8000: train loss 3.32997\n",
      "iter_dt 34.42ms; iter 8500: train loss 4.45516\n",
      "iter_dt 34.46ms; iter 9000: train loss 3.54904\n",
      "iter_dt 34.45ms; iter 9500: train loss 3.37663\n",
      "iter_dt 34.45ms; iter 10000: train loss 3.78096\n",
      "iter_dt 34.61ms; iter 10500: train loss 4.38127\n",
      "iter_dt 34.41ms; iter 11000: train loss 4.44518\n",
      "iter_dt 34.47ms; iter 11500: train loss 3.75112\n",
      "iter_dt 34.42ms; iter 12000: train loss 3.27926\n",
      "iter_dt 34.45ms; iter 12500: train loss 3.57408\n",
      "iter_dt 34.43ms; iter 13000: train loss 4.03889\n",
      "iter_dt 34.39ms; iter 13500: train loss 4.52193\n",
      "iter_dt 34.42ms; iter 14000: train loss 3.57478\n",
      "iter_dt 34.47ms; iter 14500: train loss 4.23732\n",
      "iter_dt 34.08ms; iter 15000: train loss 3.55682\n",
      "iter_dt 34.41ms; iter 15500: train loss 4.15677\n",
      "iter_dt 34.44ms; iter 16000: train loss 2.96361\n",
      "iter_dt 34.44ms; iter 16500: train loss 3.26918\n",
      "iter_dt 34.41ms; iter 17000: train loss 4.14401\n",
      "iter_dt 34.42ms; iter 17500: train loss 3.69795\n",
      "iter_dt 34.41ms; iter 18000: train loss 4.18935\n",
      "iter_dt 34.39ms; iter 18500: train loss 3.52674\n",
      "iter_dt 34.42ms; iter 19000: train loss 2.98434\n",
      "iter_dt 34.60ms; iter 19500: train loss 3.80402\n"
     ]
    }
   ],
   "source": [
    "model = GPT(config).to(config.device)\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "trainer = Trainer(config, model, train_dataset)\n",
    "\n",
    "def batch_end_callback(trainer):\n",
    "    if trainer.iter_num % 500 == 0:\n",
    "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved state_dict keys: odict_keys(['_orig_mod.transformer.wte.weight', '_orig_mod.transformer.wpe.weight', '_orig_mod.transformer.h.0.ln_1.weight', '_orig_mod.transformer.h.0.ln_1.bias', '_orig_mod.transformer.h.0.attn.c_attn.weight', '_orig_mod.transformer.h.0.attn.c_attn.bias', '_orig_mod.transformer.h.0.attn.c_proj.weight', '_orig_mod.transformer.h.0.attn.c_proj.bias', '_orig_mod.transformer.h.0.ln_2.weight', '_orig_mod.transformer.h.0.ln_2.bias', '_orig_mod.transformer.h.0.mlp.c_fc.weight', '_orig_mod.transformer.h.0.mlp.c_fc.bias', '_orig_mod.transformer.h.0.mlp.c_proj.weight', '_orig_mod.transformer.h.0.mlp.c_proj.bias', '_orig_mod.transformer.h.1.ln_1.weight', '_orig_mod.transformer.h.1.ln_1.bias', '_orig_mod.transformer.h.1.attn.c_attn.weight', '_orig_mod.transformer.h.1.attn.c_attn.bias', '_orig_mod.transformer.h.1.attn.c_proj.weight', '_orig_mod.transformer.h.1.attn.c_proj.bias', '_orig_mod.transformer.h.1.ln_2.weight', '_orig_mod.transformer.h.1.ln_2.bias', '_orig_mod.transformer.h.1.mlp.c_fc.weight', '_orig_mod.transformer.h.1.mlp.c_fc.bias', '_orig_mod.transformer.h.1.mlp.c_proj.weight', '_orig_mod.transformer.h.1.mlp.c_proj.bias', '_orig_mod.transformer.h.2.ln_1.weight', '_orig_mod.transformer.h.2.ln_1.bias', '_orig_mod.transformer.h.2.attn.c_attn.weight', '_orig_mod.transformer.h.2.attn.c_attn.bias', '_orig_mod.transformer.h.2.attn.c_proj.weight', '_orig_mod.transformer.h.2.attn.c_proj.bias', '_orig_mod.transformer.h.2.ln_2.weight', '_orig_mod.transformer.h.2.ln_2.bias', '_orig_mod.transformer.h.2.mlp.c_fc.weight', '_orig_mod.transformer.h.2.mlp.c_fc.bias', '_orig_mod.transformer.h.2.mlp.c_proj.weight', '_orig_mod.transformer.h.2.mlp.c_proj.bias', '_orig_mod.transformer.h.3.ln_1.weight', '_orig_mod.transformer.h.3.ln_1.bias', '_orig_mod.transformer.h.3.attn.c_attn.weight', '_orig_mod.transformer.h.3.attn.c_attn.bias', '_orig_mod.transformer.h.3.attn.c_proj.weight', '_orig_mod.transformer.h.3.attn.c_proj.bias', '_orig_mod.transformer.h.3.ln_2.weight', '_orig_mod.transformer.h.3.ln_2.bias', '_orig_mod.transformer.h.3.mlp.c_fc.weight', '_orig_mod.transformer.h.3.mlp.c_fc.bias', '_orig_mod.transformer.h.3.mlp.c_proj.weight', '_orig_mod.transformer.h.3.mlp.c_proj.bias', '_orig_mod.transformer.h.4.ln_1.weight', '_orig_mod.transformer.h.4.ln_1.bias', '_orig_mod.transformer.h.4.attn.c_attn.weight', '_orig_mod.transformer.h.4.attn.c_attn.bias', '_orig_mod.transformer.h.4.attn.c_proj.weight', '_orig_mod.transformer.h.4.attn.c_proj.bias', '_orig_mod.transformer.h.4.ln_2.weight', '_orig_mod.transformer.h.4.ln_2.bias', '_orig_mod.transformer.h.4.mlp.c_fc.weight', '_orig_mod.transformer.h.4.mlp.c_fc.bias', '_orig_mod.transformer.h.4.mlp.c_proj.weight', '_orig_mod.transformer.h.4.mlp.c_proj.bias', '_orig_mod.transformer.h.5.ln_1.weight', '_orig_mod.transformer.h.5.ln_1.bias', '_orig_mod.transformer.h.5.attn.c_attn.weight', '_orig_mod.transformer.h.5.attn.c_attn.bias', '_orig_mod.transformer.h.5.attn.c_proj.weight', '_orig_mod.transformer.h.5.attn.c_proj.bias', '_orig_mod.transformer.h.5.ln_2.weight', '_orig_mod.transformer.h.5.ln_2.bias', '_orig_mod.transformer.h.5.mlp.c_fc.weight', '_orig_mod.transformer.h.5.mlp.c_fc.bias', '_orig_mod.transformer.h.5.mlp.c_proj.weight', '_orig_mod.transformer.h.5.mlp.c_proj.bias', '_orig_mod.transformer.h.6.ln_1.weight', '_orig_mod.transformer.h.6.ln_1.bias', '_orig_mod.transformer.h.6.attn.c_attn.weight', '_orig_mod.transformer.h.6.attn.c_attn.bias', '_orig_mod.transformer.h.6.attn.c_proj.weight', '_orig_mod.transformer.h.6.attn.c_proj.bias', '_orig_mod.transformer.h.6.ln_2.weight', '_orig_mod.transformer.h.6.ln_2.bias', '_orig_mod.transformer.h.6.mlp.c_fc.weight', '_orig_mod.transformer.h.6.mlp.c_fc.bias', '_orig_mod.transformer.h.6.mlp.c_proj.weight', '_orig_mod.transformer.h.6.mlp.c_proj.bias', '_orig_mod.transformer.h.7.ln_1.weight', '_orig_mod.transformer.h.7.ln_1.bias', '_orig_mod.transformer.h.7.attn.c_attn.weight', '_orig_mod.transformer.h.7.attn.c_attn.bias', '_orig_mod.transformer.h.7.attn.c_proj.weight', '_orig_mod.transformer.h.7.attn.c_proj.bias', '_orig_mod.transformer.h.7.ln_2.weight', '_orig_mod.transformer.h.7.ln_2.bias', '_orig_mod.transformer.h.7.mlp.c_fc.weight', '_orig_mod.transformer.h.7.mlp.c_fc.bias', '_orig_mod.transformer.h.7.mlp.c_proj.weight', '_orig_mod.transformer.h.7.mlp.c_proj.bias', '_orig_mod.transformer.ln_f.weight', '_orig_mod.transformer.ln_f.bias', '_orig_mod.lm_head.weight'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "state_dict = torch.load('model.pth')\n",
    "print(\"Saved state_dict keys:\", state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lord:\n",
      "Rise! My people, conquer the north!\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "I pray thee, let me hear; I will not hear some of him.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "I am a man, sir, that was a man.\n",
      "\n",
      "DUKE OF YORK:\n",
      "I was, my lord, I know not what I was,\n",
      "That I may be, I fear, that have done.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "I pray thee, let me hear; but, gentle Clarence, let me speak.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "I am a king, and I am not king.\n",
      "\n",
      "YORK:\n",
      "I am not, my lord.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "I am not yet, I am not yet.\n",
      "\n",
      "YORK:\n",
      "I am not, my lord.\n",
      "\n",
      "YORK:\n",
      "I am not yet, I am not yet.\n",
      "\n",
      "YORK:\n",
      "I am not, my lord.\n",
      "\n",
      "KING RICHARD III:\n",
      "I am not king; but, as I hear,\n",
      "I am the king, the king, the king, and I,\n",
      "The king, the king, the king, the king,\n",
      "The king, the king, the king, and all,\n",
      "The king, the king, and all his son,\n",
      "With all his son, his son, his son, his son:\n",
      "And then, I hope, he is at hand.\n",
      "\n",
      "CLARENCE:\n",
      "I am too late, my lord.\n",
      "\n",
      "GLOUCESTER:\n",
      "\n",
      "KING EDWARD IV:\n",
      "I am too late, my lord.\n",
      "\n",
      "YORK:\n",
      "I am not yet, my lord.\n",
      "\n",
      "GLOUCESTER:\n",
      "\n",
      "YORK:\n",
      "I am not yet.\n",
      "\n",
      "YORK:\n",
      "I am not, my lord.\n",
      "\n",
      "GLOUCESTER:\n",
      "I am not, my lord.\n",
      "\n",
      "YORK:\n",
      "I am not yet.\n",
      "\n",
      "YORK:\n",
      "I am not, my lord.\n",
      "\n",
      "KING HENRY VI:\n",
      "I am not yet, I am not yet.\n",
      "\n",
      "YORK:\n",
      "I am not, my lord.\n",
      "\n",
      "YORK:\n",
      "I am not, my lord.\n",
      "\n",
      "KING HENRY VI:\n",
      "I am not yet, I am not yet.\n",
      "\n",
      "YORK:\n"
     ]
    }
   ],
   "source": [
    "text = 'Lord:\\nRise! My people, conquer the north!'\n",
    "sample_ids = torch.Tensor(enc.encode_ordinary(text)).long()\n",
    "sample_ids = torch.unsqueeze(sample_ids, 0).to(config.device)\n",
    "model.eval()\n",
    "result = model.generate(sample_ids, max_new_tokens=500, temperature=1, do_sample=False, top_k=None)\n",
    "print(enc.decode(result.detach().cpu().tolist()[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
